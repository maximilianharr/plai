{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Processing\n",
    "We will use ntlk (natural language toolkit) python library.  \n",
    "See: https://www.nltk.org/  \n",
    "\n",
    "\n",
    "#### Tokens\n",
    "Tokenizer split words and sentences. This is a non-trivial task.\n",
    "\n",
    "#### Stemming\n",
    "Remove ending of word.\n",
    "\n",
    "#### Lemmatizer\n",
    "Similar to stemming\n",
    "```bash\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "```\n",
    "from princeton university\n",
    "\n",
    "#### POS-Tagging\n",
    "Tag words (e.g. noun, verb, etc.)\n",
    "\n",
    "#### Dataset: Amazon Fine Food Reviews\n",
    "\n",
    "https://www.kaggle.com/snap/amazon-fine-food-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# https://www.nltk.org/data.html\n",
    "nltk.set_proxy('http://127.0.0.1:3128')\n",
    "nltk.download(\"popular\") # Download popular trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add tags to words\n",
    "text = \"He went into a supermarket in St. Petersburg. There, he bought a Knusperbroetchen for 9.99 $. He knew it better.\"\n",
    "\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    # Print words of a sentence\n",
    "    print(nltk.word_tokenize(sentence))\n",
    "\n",
    "    # Print tags of each word (e.g. adjective, past, etc)\n",
    "    tagged_words = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "    for tagged_word in tagged_words:\n",
    "        print(tagged_word[0] + \"/\" + tagged_word[1])\n",
    "    #print(nltk.pos_tag(nltk.word_tokenize(sentence))) \n",
    "\n",
    "    # \n",
    "    final_sentence = []\n",
    "    for tagged_word in tagged_words:\n",
    "        final_sentence.append(tagged_word[0] + \"/\" + tagged_word[1])\n",
    "    print(\"Final sentence: \", final_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming - Remove word endings \n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "s = SnowballStemmer(\"german\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove ending of word\n",
    "print(\"Autohäuser: \", s.stem(\"Autohäuser\"))\n",
    "print(\"gegangen: \", s.stem(\"gegangen\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizer \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "l = WordNetLemmatizer()\n",
    "print(\"going: \", l.lemmatize(\"going\", \"v\"))\n",
    "print(\"went: \", l.lemmatize(\"went\", \"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "# Function which converts nltk tags to wordnet\n",
    "# Source: https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize sentence\n",
    "words_tagged = nltk.pos_tag(nltk.word_tokenize(\"He went to his friends.\"))\n",
    "\n",
    "for word in words_tagged:\n",
    "    print(l.lemmatize(word[0], get_wordnet_pos(word[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reviews\n",
    "df = pd.read_csv(\"../res/Reviews_10000.csv.bz2\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract adjectives from reviews\n",
    "texts = df[\"Text\"]#[:1000]\n",
    "texts_transformed = []\n",
    "\n",
    "for review in texts:\n",
    "    # Split review into sentences\n",
    "    sentences = nltk.sent_tokenize(review)\n",
    "    adjectives = []\n",
    "    \n",
    "    # Split words\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        \n",
    "        # Tag words\n",
    "        words_tagged = nltk.pos_tag(words)\n",
    "        \n",
    "        for word_tagged in words_tagged:\n",
    "            \n",
    "            # Only print adjectives\n",
    "            if word_tagged[1] == \"JJ\":\n",
    "                adjectives.append(word_tagged[0])\n",
    "                #print(word_tagged)\n",
    "    \n",
    "    texts_transformed.append(\" \".join(adjectives))\n",
    "    # print(\" \".join(adjectives))\n",
    "\n",
    "#print(texts_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for machine learning\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "x = texts_transformed\n",
    "y = df[\"Score\"] >=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state = 0)\n",
    "\n",
    "# Count features and vectorize\n",
    "cv = CountVectorizer(max_features = 50) # Train 50 most common adjectives\n",
    "cv.fit(x_train)\n",
    "\n",
    "x_train = cv.transform(x_train)\n",
    "x_test = cv.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model = MultinomialNB()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "print(model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print trained adjectives\n",
    "adj = list(zip(model.coef_[0], cv.get_feature_names()))\n",
    "\n",
    "adj = sorted(adj)\n",
    "\n",
    "# Print adjectives sorted\n",
    "for i in adj:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
